Senario-
suppose we have application and large number of user is accesing the application so in this situation
  user service feels so much load on this . to overcome this problem(Amazone big sell-Lakh of user
   using application at same time )

1. Load Balancing in Microservices
   Load balancing ensures incoming traffic is distributed across multiple instances of a microservice to:
    Improve reliability
    Handle more traffic
    Avoid overloading a single instance

    If you have 3 instances of a user-service running, a load balancer (LB) will forward requests
    to all of them based on a chosen algorithm (e.g., round-robin, least-connections, etc.)

    how to create 3 instance-
    An instance of a microservice means a running copy of that service.

   1. If you have a microservice called user-service, running 3 instances means you're running 3 copies
    of that application ‚Äî possibly on different ports, servers, containers, or pods ‚Äî at the same time.
   2.  In Kubernetes (Recommended for Production)
        Kubernetes lets you define how many replicas (instances) of a microservice you want.
        Example deployment.yaml:    \
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: user-service
        spec:
          replicas: 3  # <--- 3 instances
          selector:
            matchLabels:
              app: user-service
          template:
            metadata:
              labels:
                app: user-service
            spec:
              containers:
                - name: user-service
                  image: your-docker-repo/user-service:latest
                  ports:
                    - containerPort: 8080
-------------------------------------------------------------
I understand making 3 instance . i want to know how load balancer (LB) will forward requests to
all of them based on a chosen algorithm (e.g., round-robin, least-connections, etc.)

üîÑ How Load Balancers Forward Requests to Instances
When you have multiple instances of a microservice (like user-service), a load balancer is placed in
front of them.

It receives all client requests and forwards them to the instances based on a chosen algorithm like:
| Algorithm             | What it does                                                        |
| --------------------- | ------------------------------------------------------------------- |
| **Round Robin**       | Sends requests to instances in order (1‚Üí2‚Üí3‚Üí1‚Üí...)                  |
| **Least Connections** | Sends request to the instance with the fewest active connections    |
| **Random**            | Picks an instance randomly                                          |
| **IP Hash**           | Uses client IP hash to always go to same instance (sticky sessions) |

1Ô∏è‚É£ Spring Cloud LoadBalancer (Client-side)
If you're using Spring Cloud (with Eureka), the client application itself uses a load balancer
to pick which instance to call.

üì¶ Example:@LoadBalanced
          @Bean
          public RestTemplate restTemplate() {
              return new RestTemplate(); // Uses round-robin by default
          }


restTemplate.getForObject("http://user-service/api/users", String.class);
Here:
user-service may have 3 instances
Eureka registers all 3
Spring Cloud LoadBalancer picks one (by default using Round-Robin)
‚öôÔ∏è Customize algorithm:
@Bean
ReactorLoadBalancer<ServiceInstance> loadBalancer(Environment env, LoadBalancerClientFactory factory) {
    String name = env.getProperty(LoadBalancerClientFactory.PROPERTY_NAME);
  return new RandomLoadBalancer(factory.getLazyProvider(name, ServiceInstanceListSupplier.class), name);
}

2Ô∏è‚É£ Kubernetes (Server-side Load Balancer)
In Kubernetes:
You define a Service object.
That acts as a load balancer to route traffic to pods.
üîß Example:apiVersion: v1
          kind: Service
          metadata:
            name: user-service
          spec:
            selector:
              app: user-service
            ports:
              - port: 80
                targetPort: 8080
This service forwards to all matching pods (app=user-service)
Uses Round-Robin by default
Kubernetes load balances at network level.

------------------------------------------
if i want to implement Least Connections-Sends request to the instance with the fewest active connections
NGINX Load Balancer (Recommended for Least Connections)-
üß± NGINX Config:http {
                   upstream user_backend {
                       least_conn;
                       server 127.0.0.1:8080;
                       server 127.0.0.1:8081;
                       server 127.0.0.1:8082;
                   }

                   server {
                       listen 80;

                       location / {
                           proxy_pass http://user_backend;
                       }
                   }
               }
‚úÖ What This Does:
least_conn tells NGINX to forward requests to the backend with the fewest active connections.
You run 3 instances of user-service on ports 8080, 8081, 8082.

NGINX is a web server that also acts as a powerful reverse proxy, load balancer, and HTTP cache.

NGINX Load Balancer = A setup where NGINX forwards incoming client requests to multiple backend
servers (microservice instances) based on a configured algorithm (like round-robin or least-connections).
Advantages of NGINX-
1.NGINX can improve can compress the response
2.Sending the data in chunks
üì¶ Where NGINX fits in a microservices architecture:
Client ‚Üí NGINX (Load Balancer) ‚Üí Multiple Instances of user-service
                                     ‚îú‚îÄ‚îÄ http://127.0.0.1:8080
                                     ‚îú‚îÄ‚îÄ http://127.0.0.1:8081
                                     ‚îî‚îÄ‚îÄ http://127.0.0.1:8082
üîÅ How It Works
You run 3 instances of your Spring Boot user-service on ports 8080, 8081, and 8082.

NGINX listens on port 80.

When a client hits http://localhost/, NGINX chooses one of the 3 instances based on the algorithm
 (least_conn above) and forwards the request.




